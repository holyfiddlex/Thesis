@ARTICLE{Cherry,
TITLE     = {\href{https://www.google.com}{Some Experiments on the Recognition of Speech, with One and with Two Ears}},
AUTHOR    = {C E Colin},
JOURNAL   = {The Journal of the Acoustical Society of America},
VOLUME    = {25},
NUMBER    = {5},
PAGES     = {975--979},
YEAR      = {1953},
}
@INPROCEEDINGS{CocktailPartyProblemRevisit,
TITLE     = {\href{https://www.google.com}{The Role of Top-Down Attention in the Cocktail Party: Revisiting Cherry’s Experiment after Sixty Years}},
BOOKTITLE = {Proceedings of the tenth International Conference on Machine Learning and Applications (ICMLA'11)},
ABSTRACT  = {We investigate the role of top-down task drive attention in the cocktail party problem. In a recently proposed computational model of top-down attention it is possible to simulate the cocktail party problem and make predictions about sensitivity to confounders under different levels of attention. Based on such simulations we expect that under strong top-down attention pattern recognition is improved as the model can compensate for noise and confounders. We next investigate the role of temporal and spectral overlaps and speech intelligibility in humans, and how the presence of a task influences their relation. For this purpose, we perform behavioral experiments inspired by Cherry’s classic experiments carried out almost sixty years ago. We make participants listen to a mono signal consisting of two different narratives pronounced by a speech synthesizer under two different conditions. In the first case, participants listen with no specific task, while in the second one they are asked to follow one of the stories. Participants report the words they heard by choosing from a list which also includes terms not present in any of the narratives. We define temporal and spectral overlaps using the ideal binary mask (IBMs) as a gauge. We analyze the correlation between overlaps and the amount of reported words. We observe a significant negative correlation when there is no task, while no correlation is detected when a task is involved. Hence, results that are well aligned with the simulation results in our computational top-down attention model.},
AUTHOR    = {Letizia Marchegiani and Seliz Karadogan and Tobias Andersen and Jan Larsen and Hansen, {Lars Kai}},
LANGUAGE  = {English},
PUBLISHER = {IEEE},
ADDRESS   = {United States},
YEAR      = {2011},
}
@BOOK{DeepLearning,
TITLE     = {\href{http://www.deeplearningbook.org}{Deep Learning}},
AUTHOR    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
PUBLISHER = {MIT Press},
YEAR      = {2016}
}