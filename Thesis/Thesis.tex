\documentclass{book}
\usepackage{color}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[toc,page]{appendix}
\usepackage[left=4cm, right=4cm, top=3cm]{geometry}

\hypersetup{colorlinks=true, linkcolor=blue}
\hypersetup{urlcolor=blue}
\hypersetup{citecolor=red}

\setlength{\parindent}{2em}
\setlength{\parskip}{1em}
\setcounter{chapter}{-1}


\begin{document}

\newcommand{\titlename}{Data Processing for the Cocktail Party Problem}

\begin{titlepage}
    \begin{center}
        \vspace*{1cm}
 
        \Huge
        \textbf{\titlename}
 
        \vspace{0.5cm}
        \LARGE
        A review of **
 
        \vspace{1.5cm}
 
        \textbf{Héctor M. de la Rosa Prado}
 
        \vfill
 
        A thesis presented for the degree of\\
        Bachelor in IT for Science
 
        \vspace{0.8cm}
 
        % \includegraphics[width=0.5\textwidth]{../images/unam.png}
        
        \Large
        Department Name **\\
        National Autonomous University of Mexico\\
        Mexico\\
        \today\\
 
    \end{center}
\end{titlepage}
\tableofcontents
\newpage
\thispagestyle{plain}

\chapter*{}
\addcontentsline{toc}{chapter}{Acknowledgements}

\chapter*{}
\addcontentsline{toc}{chapter}{Abstract}
\begin{center}
    \Large
    \textbf{\titlename}
 
    \vspace{0.4cm}
    \large
    Thesis Subtitle
 
    \vspace{0.4cm}
    \textbf{Héctor M. de la Rosa Prado}
 
    \vspace{0.9cm}
    \textbf{Abstract}
\end{center}

\chapter*{Preface}
\addcontentsline{toc}{chapter}{Preface}
\qquad This work is the dissertation "\titlename" in which we explore the current preprocessing methods used in solving the Cocktail Party Problem oriented in audio.
It was written to fulfill the graduation requirements for the bachelor degree in IT for Science.
\chapter{Introduction}
\section{The Cocktail Party Problem}
\qquad Imagine being in a public area with one of your friends.
As you talk about your work and what you learned in school there are people around you making noise.
They’re also talking about their lives, what they plan on doing in the mall or the movie that just came out.
Kids laughing with their parents talking, nearby traffic with car horns flaring in the distance.
If you wanted to, you could hear them, but instead you ignore it.
\par
You continue to listen to your friend, regardless of the noise in the background.
You have no problem paying attention until suddenly you hear your name being mentioned in the crowd.
For some reason, somewhere, someone said your name.
You look around to see a familiar face walking towards you…
\par
What we described in the short and imaginary story above is what is called the “Cocktail Party Effect”.
This effect was first coined by Cherry Colin\cite{Cherry} where he proposed a series of tests that would measure the limits of a human’s ability to listen to a specific voice under different circumstances.
The problem of getting a machine to do this same task was called the Cocktail Party Problem by Cherry.
\par
The cocktail party problem is one of the biggest unsolved problems in computation.
The short and imaginary story above gives us an example of how we, as humans, solve this problem in a seemingly effortless way.
Thanks to evolution, humans can it so effortless, in fact, that most people don’t even stop to appreciate how complicated the task actually is.
\par
Its difficulty, however, is not the reason it is so widely studied.
In fact, this small problem seems to be the barrier that has kept us from advancing in the automation of automation, or at least in the way we would like.
The cocktail party is not only present in sound, but in just about any signal processing problem, from medical scanning to telecommunications\cite{CocktailPartyProblemRevisit}, noise always seems to find its way into our sensors.
\par
That is why a general solution to this problem will not only allow us to improve greatly in audio related tasks, such as speech recognition, transcriptions, audio classification, and audio/speech enhancements, but also in various fields like medical analysis and seismology.
The reach of these advancements also promises a wide range of new technologies shortly after.
\par
This is why, in this work, we will attempt to take a dive into how we can leverage the current improvements in artificial intelligence in tackling the problem.
Ever since the latest boom of deep learning \cite{DeepLearning}
\par
Even though, as we saw before, the problem generalizes beyond speech, we will limit our research to stick with voiced data.
This is due to the complexity in examining signals as a whole, and because of the focus that most methods currently have on the subject.
\par
Given how the difficulty of the problem lays much beyond my current limits as a researcher we will not show.
We will accept limiting our problem greatly for practical reasons, in an attempt to get insight on .
This also includes a review of how traditional methods attempt to solve the problem.
\section{History}
\qquad Speech separation has been a problem that researchers have been interested in for years.
So much so that the problem was formulated decades ago by Colin Cherry\cite{Cherry}.
In his famous paper he gives an example of the task with a conversation in a Cocktail party, giving the problem its name.
Over 65 years later one could argue that progress is just now being made in the area, mostly due to the advancements in general learning algorithms, like deep learning, giving an edge in unstructured data analysis.
\section{Document Structure}
\qquad In this project we will separate our investigation into three parts.
The first part will review the current literature on the problem, focusing specifically on current processing methods.
We then explore how previous studies used these methods and the possible affects this has on the results.
This will be split into 4 Chapters.
\par
Chapter 1 will give a short rundown on the current data processing techniques.
We will explain why data preprocessing is important and what problems occur without it, focusing on audio tasks.
This chapter is divided into two parts, the first will touch on feature extraction for audio.
The second part will touch on Normalization.
\par
For feature extraction we will need to learn about Spectrograms.
The Spectrograms can be separated into two commonly used categories: linear and mel-bin.
Both spectrograms represent audio clips into a 3-D transformations, similar to images.
A linear spectrogram has a linear frequency dimension while mel-bins represents frequency in logarithmic scale.
\par
Chapter 2 will reference one of the problems we currently have in Generating Audio.
Here we explain why generating audio directly isn't always possible and what the modern solution for these problems currently looks like.
The critiques given to these current solutions will be based on the literature and on my own personal experience.
\par
Chapters 3 and 4 will talk about recent projects.
Although these projects do not tackle the problem in the same way, they have reasonably good quality in audio.
Because of this, we will use these studies as a reference to what practices could be used to provide these results.
Later on we will see that one uses vocoders and the other changes the training target.
\par
At the end of these chapters we will state the pros and cons of each methods and briefly show the train of thought that lead to the experiments that we can find in part 2.
\par Part two contains the methodology that was taken in this study.
In this we hope to explain the experiment well enough to be replicated in future studies.
For this we provide a context of the algorithm and how each process was implemented.
As we will learn later, the implementation of some of these algorithms which varies in the different libraries affect the outputs similar to the parameters.
\par
Finally the third and final part of this text will give the results to the project.
In this we will explain what the quantified errors represent and a quick qualitative assessment of the audios.
***As a reminder, we do not plan on building a model that competes against the current SoTA.
***Instead we will attempt to find input and target creation techniques that improve the quality of the generated audio.
\par
The final part of this thesis contains the appendix and the bibliography.
In the appendix we give most of the theoretical background necessary to understand the methods used in our research.
This includes subjects ranging from machine learning, signal processing, the fourier and wavelet transform and a small usage guide for the source code and where to find it.
\part{Literature Review}
\chapter{Speech Separation}
\section{Segmentation vs Attention Problem}
\qquad Many studies** have been used to divide the problem into more manageable tasks, many of which were included in the experiments made by Cherry himself.
Thanks to this, the problem has evolved quite a bit since it was first formulated, deviating from its original definition and including all types of noise, not just speech.
As McDermott mentions in his article\cite{CocktailPartyProblemRevisit}, the problem has been split in two parts, which together engulfs the problem as a whole.
Usually both parts are frequently referred to as the Cocktail Party problem interchangeably.
\par
The first part addresses who or what to pay attention to, and when to change targets.
For clarity in this paper, we will refer to this specific part as the Attention problem.
To solve this problem we need background knowledge about the person paying attention as well as the context to know what the listener wants to listen to.
What's more is that the listener might want to listen to two speakers at the same time, given that they are both saying something important.
The difficulty of the problem lies in how subjective it is.
Because of this, it is practically impossible to model and automate the process.
\par
The second part refers to actually separating the sounds, or also known as “sound segregation”.
This part will be referred to as the “segregation problem.
This differs from the previous task because here we look to separate the audio by different sources.
We have no need to know which is more important than the other, as long as we can listen to each source independently.
This is the process that has received more attention in the last couple of years, since it is easier to model.
That said, it is still an ill-posed problem (more on this later) which means that traditional methods have a hard time.
\par
In this thesis we will only be addressing the segregation problem, trying to separate an audio by the different sources of noise it contains.
This problem has quite a bit of research ** but still has a long way before we can actually consider it to be solved.
One of the reasons was just mentioned, but it’s also due to the fact that the problem, under certain conditions, becomes impossible to model.
This leaves us with a few options, we either limit the problem to certain use cases, or we facilitate the problem adding more information.
We will consider both of these actions to be “constraints”, as one limits the conditions that could be solved while the other limits possible solutions, requiring more information.

\section{Inverse Problems}
\qquad An inverse problem refers to reconstructing the source from a generated set of components.
In this case, we want to reconstruct the separated audios from a generated mixed audio.
Inverse problems are usually classified by how “Well-Posed” it is, which is also generally used as an indicator of how difficult the problem could be.
Many examples of Inverse problems that extend to signal processing include medical imaging, oceanography, astronomy and geophysics. [InverseProblems]
\par
An inverse problem has 4 components, this includes a measurement operator (MO), injectivity or regularization of the MO, knowledge on noise and the model and finally a numerical implementation.
Some of these components are then separated into different sub-catagories, but we will only mention the ones relevant to this work.
We address the components based on Guillaume Bal’s definition [InverseProblems].

\begin{center}
\noindent Measurement Operator
\end{center}
\begin{itemize}
    \item 1
    \item 2
    \item 3
\end{itemize}

\begin{center}
\noindent Injectivity
\end{center}
\begin{itemize}
    \item 1
    \item 2
    \item 3
\end{itemize}

\begin{center}
    \noindent Noise and Model
\end{center}
\begin{itemize}
    \item Noise is practically inevitable in audio, extending from white to brown noise and most times even “structured” noise.
    This could come from background music or oncoming traffic.
    When we record in a crowded room the noise changes too, leaving a sound that we can relate with a busy street.

    \item The measurement will also have errors, but these are less relevant to us.
    In audio compression, there is always a slight error from the original source to what we hear later on.
    This, however, is such a small change that we as humans cannot perceive it, at least not without the help of special tools.

    \item **
\end{itemize}
\begin{center}
Numerical Implementation
\end{center}
    For this project we will be using an optimization method, comparing it to previous linear systems.
    Although these methods were usually considered fairly inexpensive, we will be using neural networks, which are both expensive computational as well as in the data sense. 


\section{Ill-Posed Problems}
\qquad Let's imagine that we have an audio with two sources that generate noise.
One is a dog, and the other is a human.
Both audio have different structures and can be separated by a person with quite little effort.
We could even separate the audios using certain statistical methods**.
Sadly the Cocktail Party includes problems much more complicated than the one we just mentioned.
A wide variety of audios include at least two people talking.
This is harder to separate, sometimes even for humans, especially if the two individuals have similar voices and speak in a similar manner.
\par
    The difficulty of the Cocktail Party problem is that it is Ill-Posed.
    The terms Ill and Well-Posed problems was first coined by Hadamard, Jacques, a French mathematician born in the mid 1800’s.
    According to his definition, a problem is considered to be Well-Posed if: 
\begin{itemize}
    \item For any input there is a solution
    \item That solution is unique
    \item The solution is “stable” in the input space (continuous)
\end{itemize}
If one of these three conditions were not met, the problem was considered Ill-Posed.
At the time, it was considered necessary for any mathematical problem in physics and tech to be formulated as a well-posed problem.
This meant that, for practical uses, studying Ill-Posed problems was useless[Ill-Posed].

Now, however, Ill-Posed problems are very common, in fact it is considered rare that a inverse-problem should be well-posed.
It has come to the point where researchers seemingly compare how “Ill-Posed” a problem may be[InverseProblems].
These extends to Well, mildly Ill and severely Ill-Posed problems.

\section{Constraints}
\qquad If we continue with our thought experiment from before, what would happen if instead we had two machines that emit static noise of the same type, same volume similar position etc.
This would be practically impossible to separate without knowing more about the noise being emitted to try and find some structural difference in the two.
Not only that, but the problem wouldn’t be useful in this broader sense.
Why would you want to separate noise from noise? The situations where we want to solve the problem don’t require us to tackle a perfectly generalized version of it.
This is why it is common to set constraints to the problem before beginning to train a model.
\par
As mentioned before, this work is about speech separation, so it is fair to assume that the audio will have at least two people talking.
Another constraint we will add is that we will only focus on english speakers, leaving a more general model for another project.
As we will see below we will also be using the Common Voice dataset.
This dataset is publicly available to make similar models and consists of almost 40000 voices in more than 1000 hours of recorded english voice in different accents.
\par
We could also consider the number of speakers as a constraint, since having only 2 speakers simplifies the problem greatly.
During these tests** we will try to make a general model that can split the different speakers even when mixed with an arbitrary amount of voices.
Clearly there will be a limit given how 100 voices will more than likely be worse than gibberish, but this arbitrary model could help us separate audio even when unexpected speakers enter the conversation.
\par
Another constraint that could be considered is that we only need to listen to one voice.
If you use a hypothetical “perfect” model to separate voices and listen to your friend in a crowded cocktail party, you have little to no interest in listening to other people's conversations.
Although this could definitely be easier and give better results, we go back to what we mentioned before when we introduced the cocktail party.
This constraint in reality mentions a problem with attention, which for now we are considering a different problem.

\part{Methodology}
\chapter{Literature Review}
  \section{Current SoTA with AI}
    \subsection{Speech Recognition}
    \subsection{Speech Separation}
      \subsubsection{Looking to Listen}
  \section{Feature Extraction and Normalization}
    \subsection{Do Spectrograms belong in AI?}
  \section{Data Augmentation}
  \section{Targets and Masks}

\chapter{Methodology}
  \section{Computer Requirements}
  \section{Dataset}
  \section{Preprocessing}
  \section{Model}
  \section{Targets}
  \section{Error}

\chapter{Implementation}
  \section{Libraries}
    \subsection{Librosa}
    \subsection{PyTorch}
    \subsection{TorchAudio}
  \section{Source Code}
    \subsection{Running Instructions}


\part{Results}
\chapter{Results}
  \section{Initial Dataset}
    \subsection{Quantified Losses}
    \subsection{Quality Tests}
    \subsection{Samples (images and audios)}
  \section{Transfer learning}
    \subsection{Quantified Losses}
    \subsection{Quality Tests}
    \subsection{Samples (images and audios)}
  \section{Transfer Learning with fine tuning}
    \subsection{Quantified Losses}
    \subsection{Quality Tests}
    \subsection{Samples (images and audios)}

\chapter{Discussion}
  \section{Future Work}

\begin{appendices}
    \chapter{Time Series}
      \section{Signal Processing}
      \section{Speech Properties}
    
    \chapter{Transforms}
      \section{Fourier Transform}
        \subsection{Spectrums}
        \subsection{Discrete Fourier Transform}
        \subsection{Short-Time Fourier Transform}
      \section{Wavelet}
    
    \chapter{Spectrograms}
    \qquad As we saw before, audio can be better represented as mix of different frequencies that define the sound we are hearing. The problem lies in the fact that, in speech, these frequencies change drastically during each conversation, sentence, word and syllable! This means that the perfect spectrogram would need to be a continuous representation of each moment of time and for every frequency.
    \par
    This is impossible for various reasons, one being the limitations of computational representations, which forces us to make a discrete representation instead. But even if that wasn’t the case, the fourier transform doesn’t work on single points in time, instead it works on entire signals. That said, we can use the short time fourier transform to give us a spectrogram that is discrete in time. Combined with the discrete fourier transform, we will also have a discrete representation in the frequency axis.
    \par
    Spectrograms have 3 dimensions, similar to how images are represented, but not exactly. In any given image there are two dimensions that represent space and one that represents intensity. Meanwhile Spectrograms have one dimension for time, one for frequency and the last represents the strength and sometimes the phase of the frequency signal at that time. Although some writers, the standard tends to place frequency as height, time as width and color as signal intensity in a given point.
    \par
    Although this is a great step forward to giving us a better representation of audio, there are still too many parameters that play a role. Each one of these parameters affects how well the spectrogram can correctly contain the information in the audio. Although there are already well established defaults for most human tasks, it still hasn’t been well studied in great part of automated and machine learning tasks.
    \par
    The exception to this being Speech Recognition, but even so they haven’t been heavily studied. There are few papers and datasets that can be used to prove when some parameters are better than others or if there are conservitive settings that tend to work consistently in all problems. In the end, some authors question if spectrograms are even the representation that we are looking for for various reasons that we will see in more detail later in Chapter 7.
    \par
    For now we'll hide our skepticism and focus on the advantages of using spectrograms. The spectrograms we have been referring to so far are what are called linear-spectrograms. Although these are the simplest and easiest to implement, it is not the only one. We will mention the difference between Amplitude and Decibel Spectrograms as well as Frequency and Mel-Bin Spectrograms.
      \section{Linear and Log}
    \qquad The values in amplitude are represented in the “color” dimension, giving us the intensities that we see in the spectrogram image. As we saw in the Second Chapter, the amplitude tells us the strength of the signal. In the spectrogram, however, there may also be a phase value. This is because the signals tend to be represented as a Complex number, which can be expressed in cartesian or polar coordinates.
    \par
        \subsection{Amplitude vs Decibels}
        \subsection{Mel-Bins}
      \section{The Phase Problem}
        \subsection{Linear}
        \subsection{Decibel}
        \subsection{Mel-bin}
      \section{Phase Retrieval Techniques}
        \subsection{Phase Storage}
        \subsection{Griffin-lin Algorithm}
        \subsection{Vocoders}
    
    \chapter{Machine Learning}
    \qquad "A machine learning algorithm is an algorithm that is able to learn from data."\cite{DeepLearning}
    These algorithms search for patterns in the data to discover rules of association that could be used to solve the problem without explicit instructions.
    This approach turns out to be better, in various task, than previous hard-encoded methods.
    These older algorithms are usually referred to as traditional methods.
    \par
    Most of the success of machine learning systems stem from the fact that it is hard to give a perfect definition of something.
    To achieve this we would be required to tumble into epistemological questions.
    Most would agree that a definition of a cat would be incomplete without mentioning a tail, yet a cat without a tail is still a cat.
    \section{Supervised vs Unsupervised Learning}
    \qquad Machine Learning algorithms are commonly separated into supervised and unsupervised learning.
    Even though each of these methods have their pros and cons, most of the current study has focused on developing supervised methods.
    Part of the reason is because of how data is currently being collected in the real world.
    \par
    Apart from these two branches of machine learning there is another call "semi" supervised learning\cite{SemiSupervised}.
    This approach uses a mix of labeled and unlabeled data items.
    In real life this allows users to quickly add labels to a dataset.
    This is done with unsupervised methods, which are later "corrected" by a human user which labels the faulty items. 
    \par
    Even though these tools have proven useful in certain problems, including large scale dataset labeling, they are not relevant to this study.
    \subsection{Supervised Learning}
    \qquad Supervised learning methods, as we mentioned before, use labeled datasets as training data.
    This allows the algorithm to repeatedly find patterns which could allow it to gain better insights.
    The training process for these algorithms is similar to a student studying with a mock exam.
    \par
    In this analogy, our mock exam is the same as our dataset.
    Even so, just like in real life, we elaborating a mock exam isn't always easy. How do we provide questions similar to the real test, without making it too similar?
    If you told a class of students that the real exam will be exactly the same as the mock, the students would be less inclined to learn the actual concepts necessary to pass the test (or any similar test).
    Instead, they might try to memorize the answers knowing that this would be enough.
    \par
    This same login is what happens when training an algorithm.
    They don't have a notion of what they are trying to learn but instead what they want to answer.
    A program is (at least until now) unable to understand that the dataset is just a mock, and that the real test comes after.
    What's worse, while a normal student would have problems memorizing tests for very long, a computer store these answers with no additional punishment.
        \subsection{Unsupervised Learning}
      \section{Advancements}
        \subsection{AREAS**}
      \section{Problems}
        \subsection{Data}
        \subsection{Interpretability}
        \subsection{Overfitting}
        \subsection{Hyper-parameters}
        \subsection{Computation}
    
    \chapter{Deep Learning}
      \section{Neural Networks}
        \subsection{Structure}
          \subsubsection{Inputs}
          \subsubsection{Weights}
          \subsubsection{Bias}
        \subsection{Back-Propagation Algorithm}
          \subsubsection{Forward Propagation}
          \subsubsection{Gradients}
      \section{Image Processing}
        \subsection{Convolutional Neural Networks}
        \subsection{Attention}
      \section{Segmentation}
        \subsection{Medicine}
        \subsection{U-Net}
\end{appendices}

\bibliography{Thesis}
\bibliographystyle{ieeetr}
\end{document}